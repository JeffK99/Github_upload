{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IOD_Lab-9_6_jk.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYvQOebqLcfM"
      },
      "source": [
        "<div>\n",
        "<img src=https://www.institutedata.com/wp-content/uploads/2019/10/iod_h_tp_primary_c.svg width=\"300\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JWvLBewLcfP"
      },
      "source": [
        "# Lab 9.6: Sentiment Analysis\n",
        "INSTRUCTIONS:\n",
        "- Run the cells\n",
        "- Observe and understand the results\n",
        "- Answer the questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbhmKC6NLcfS"
      },
      "source": [
        "Based on the video tutorial **Text Classification with Machine Learning,SpaCy and Scikit(Sentiment Analysis)** by **Jesse E. Agbe (JCharis)**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnuAMgbhLcfV"
      },
      "source": [
        "## Data Source: UCI\n",
        "### UCI - Machine Learning Repository\n",
        "- Center for Machine Learning and Intelligent Systems\n",
        "\n",
        "The [**UCI Machine Learning Repository**](http://archive.ics.uci.edu/ml/about.html) is a collection of databases, domain theories, and data generators that are used by the machine learning community for the empirical analysis of machine learning algorithms.\n",
        "\n",
        "### Dataset\n",
        "- [Sentiment Labelled Sentences Data Set](http://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences)\n",
        "\n",
        "### Abstract\n",
        "The dataset contains sentences labelled with positive or negative sentiment.\n",
        "\n",
        "- Data Set Characteristics: Text\n",
        "- Number of Instances: 3000\n",
        "- Area: N/A\n",
        "- Attribute Characteristics: N/A\n",
        "- Number of Attributes: N/A\n",
        "- Date Donated: 2015-05-30\n",
        "- Associated Tasks: Classification\n",
        "- Missing Values? N/A\n",
        "- Number of Web Hits: 102584\n",
        "\n",
        "### Source\n",
        "Dimitrios Kotzias dkotzias '@' ics.uci.edu\n",
        "\n",
        "### Data Set Information\n",
        "This dataset was created for the Paper 'From Group to Individual Labels using Deep Features', Kotzias et. al,. KDD 2015\n",
        "\n",
        "Please cite the paper if you want to use it :)\n",
        "\n",
        "It contains sentences labelled with positive or negative sentiment.\n",
        "\n",
        "### Format\n",
        "sentence &lt;tab&gt; score &lt;newline&gt;\n",
        "\n",
        "### Details\n",
        "Score is either 1 (for positive) or 0 (for negative)\n",
        "\n",
        "The sentences come from three different websites/fields:\n",
        "- imdb.com\n",
        "- amazon.com\n",
        "- yelp.com\n",
        "\n",
        "For each website, there exist **500 positive** and **500 negative** sentences. Those were selected randomly for larger datasets of reviews.\n",
        "\n",
        "We attempted to select sentences that have a clearly positive or negative connotaton, the goal was for no neutral sentences to be selected.\n",
        "\n",
        "For the full datasets look:\n",
        "\n",
        "- **imdb**: Maas et. al., 2011 _Learning word vectors for sentiment analysis_\n",
        "- **amazon**: McAuley et. al., 2013 _Hidden factors and hidden topics: Understanding rating dimensions with review text_\n",
        "- **yelp**: [Yelp dataset challenge](http://www.yelp.com/dataset_challenge)\n",
        "\n",
        "\n",
        "### Attribute Information\n",
        "The attributes are text sentences, extracted from reviews of products, movies, and restaurants\n",
        "\n",
        "### Relevant Papers\n",
        "**From Group to Individual Labels using Deep Features**, Kotzias et. al,. KDD 2015\n",
        "\n",
        "### Citation Request\n",
        "**From Group to Individual Labels using Deep Features**, Kotzias et. al,. KDD 2015"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abNvVWdlLcfW"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-06-17T01:27:26.865620Z",
          "start_time": "2019-06-17T01:27:24.368522Z"
        },
        "id": "4BJWjM0zLcfZ"
      },
      "source": [
        "## Import Libraries\n",
        "import pandas as pd\n",
        "\n",
        "import regex as re\n",
        "import spacy\n",
        "import nltk\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import auc\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dzzk6JdcLcfh"
      },
      "source": [
        "## Load data\n",
        "\n",
        "Load Yelp, Amazon and Imdb Data.\n",
        "\n",
        "Hint: Source is separated by <tab>s and has no headers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-06-17T01:29:38.157718Z",
          "start_time": "2019-06-17T01:29:38.152747Z"
        },
        "id": "GZUWhcCuLcfi"
      },
      "source": [
        "yelp_text = 'yelp_labelled.txt'\n",
        "imdb_text = 'imdb_labelled_fixed.txt'\n",
        "amazon_text = 'amazon_cells_labelled.txt'\n",
        "data = [yelp_text, imdb_text, amazon_text]\n",
        "# ANSWER"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwa3MBrwLcfo"
      },
      "source": [
        "## Inspect the data\n",
        "\n",
        "Check your datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-06-17T01:30:01.495935Z",
          "start_time": "2019-06-17T01:30:01.492941Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NddGh-EQLcfq",
        "outputId": "1bb3cf89-adde-4279-a728-20c7aeaa8149"
      },
      "source": [
        "# ANSWER\n",
        "dataset = {'yelp':'','imdb':'','amazon':''}\n",
        "tag = {0:'text',1:'sentiment'}\n",
        "i = 0\n",
        "for key in df:\n",
        "  dataset[key] = pd.read_csv(data[i], delimiter = '\\t', header = None)\n",
        "  dataset[key] = dataset[key].rename(mapper=tag,axis=1)\n",
        "  i += 1\n",
        "   \n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'amazon': '',\n",
              " 'imdb': '',\n",
              " 'sentiment':                                                   text  sentiment\n",
              " 0    A very, very, very slow-moving, aimless movie ...          0\n",
              " 1    Not sure who was more lost - the flat characte...          0\n",
              " 2    Attempting artiness with black & white and cle...          0\n",
              " 3         Very little music or anything to speak of.            0\n",
              " 4    The best scene in the movie was when Gerardo i...          1\n",
              " ..                                                 ...        ...\n",
              " 995  I just got bored watching Jessice Lange take h...          0\n",
              " 996  Unfortunately, any virtue in this film's produ...          0\n",
              " 997                   In a word, it is embarrassing.            0\n",
              " 998                               Exceptionally bad!            0\n",
              " 999  All in all its an insult to one's intelligence...          0\n",
              " \n",
              " [1000 rows x 2 columns],\n",
              " 'text':                                                   text  sentiment\n",
              " 0                             Wow... Loved this place.          1\n",
              " 1                                   Crust is not good.          0\n",
              " 2            Not tasty and the texture was just nasty.          0\n",
              " 3    Stopped by during the late May bank holiday of...          1\n",
              " 4    The selection on the menu was great and so wer...          1\n",
              " ..                                                 ...        ...\n",
              " 995  I think food should have flavor and texture an...          0\n",
              " 996                           Appetite instantly gone.          0\n",
              " 997  Overall I was not impressed and would not go b...          0\n",
              " 998  The whole experience was underwhelming, and I ...          0\n",
              " 999  Then, as if I hadn't wasted enough of my life ...          0\n",
              " \n",
              " [1000 rows x 2 columns],\n",
              " 'yelp': ''}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meEtfGfELcf4"
      },
      "source": [
        "## Merge the data\n",
        "\n",
        "Merge all three datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-06-17T01:30:37.302897Z",
          "start_time": "2019-06-17T01:30:37.299903Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "WVpAx-HHcbwn",
        "outputId": "2dbf3311-c69c-4a72-ee59-424249eac778"
      },
      "source": [
        "# ANSWER\n",
        "df = pd.concat([dataset['yelp'], dataset['imdb'], dataset['amazon']])\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-d04b19bf3d1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# ANSWER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'yelp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'imdb'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'amazon'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m     )\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    357\u001b[0m                     \u001b[0;34m\"only Series and DataFrame objs are valid\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m                 )\n\u001b[0;32m--> 359\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m             \u001b[0;31m# consolidate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: cannot concatenate object of type '<class 'str'>'; only Series and DataFrame objs are valid"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBIFtbMALcf8"
      },
      "source": [
        "## Export the data\n",
        "\n",
        "Export merged datasets to as csv file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-06-17T01:31:16.041727Z",
          "start_time": "2019-06-17T01:31:16.037738Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8OLkaALLcf9",
        "scrolled": false,
        "outputId": "c96ac2a9-434c-4c8c-912d-786410d14015"
      },
      "source": [
        "# ANSWER\n",
        "\n",
        "df.isna().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "text         0\n",
              "sentiment    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdUYAIQmUd7S",
        "outputId": "2d34d7a7-6967-40b6-bb00-d7a911a88289"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 3000 entries, 0 to 999\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   text       3000 non-null   object\n",
            " 1   sentiment  3000 non-null   int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 70.3+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jvls1HIDUmeX"
      },
      "source": [
        "df.to_csv('sentiment_text.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzA4FQsPLcgA"
      },
      "source": [
        "## Prepare the stage\n",
        "- Load spaCy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-06-17T01:31:19.686599Z",
          "start_time": "2019-06-17T01:31:18.952239Z"
        },
        "id": "wVMTSDYQLcgB"
      },
      "source": [
        "nlp = spacy.load('en')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YguMrtDuLcgD"
      },
      "source": [
        "## Prepare the text\n",
        "All the text handling and preparation concerned with the changes and modifications from the raw source text to a format that will be used for the actual processing, things like:\n",
        "- handle encoding\n",
        "- handle extraneous and international charaters\n",
        "- handle simbols\n",
        "- handle metadata and embeded information\n",
        "- handle repetitions (such multiple spaces or newlines)\n",
        "\n",
        "Clean text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-06-17T01:31:31.608285Z",
          "start_time": "2019-06-17T01:31:31.601306Z"
        },
        "id": "GlsKSvonLcgD",
        "scrolled": true
      },
      "source": [
        "def clean_text(text):\n",
        "    # reduce multiple spaces and newlines to only one\n",
        "    text = re.sub(r'(\\s\\s+|\\n\\n+)', r'\\1', text)\n",
        "    # remove double quotes\n",
        "    text = re.sub(r'\"', '', text)\n",
        "\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-06-17T01:32:56.768268Z",
          "start_time": "2019-06-17T01:32:56.765283Z"
        },
        "id": "upPa3YmmLcgF"
      },
      "source": [
        "# ANSWER\n",
        "cleaned_text = df.text.apply(clean_text)\n",
        "df.text = cleaned_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "za_6vt3MLcgH"
      },
      "source": [
        "## Work the text\n",
        "Concern with the meaning and the substance of the content to extract actual information.\n",
        "\n",
        "Hint: Use techniques learned in previous labs. Remove StopWords, Punctuation, Lemmatize etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUC5udbqVRbt",
        "outputId": "f9227ada-c183-4f3f-9f8d-0f6727a95a8b"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujWRb-1PVMgM"
      },
      "source": [
        "text = 'I can barely ever hear on it and am constantly... gleefully'\n",
        "stopWords = nltk.corpus.stopwords.words('english')\n",
        "wl = nltk.stem.WordNetLemmatizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-06-17T01:32:58.911623Z",
          "start_time": "2019-06-17T01:32:58.897659Z"
        },
        "id": "sh_uDWcCLcgI"
      },
      "source": [
        "def convert_text(text):\n",
        "    '''\n",
        "    Use techniques learned in previous labs. Remove StopWords, Punctuation, Lemmatize etc.\n",
        "    '''\n",
        "    text = re.sub(r\"[\\(\\).!,?:-=+!@#'$%^&*;]+\", '', text)\n",
        "    text = re.sub(r\"-+\", '', text)\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    text = ' '.join([new_token.lower() for new_token in tokens if new_token.lower() not in stopWords])\n",
        "    text = ' '.join([wl.lemmatize(word) for word in text.split()])\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-06-17T01:33:42.014624Z",
          "start_time": "2019-06-17T01:33:01.620538Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vDv55U1LcgK",
        "outputId": "1bb0774b-4c0c-4f99-8cc0-eb4def5b2921"
      },
      "source": [
        "%%time\n",
        "df['short'] = df['text'].apply(convert_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2.56 s, sys: 136 ms, total: 2.69 s\n",
            "Wall time: 2.67 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-06-17T01:35:13.381487Z",
          "start_time": "2019-06-17T01:35:13.362526Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "faiuJfunLcgM",
        "outputId": "124f7878-8959-4670-f5c0-fe6aa82316ea"
      },
      "source": [
        "df.sample(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>short</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>873</th>\n",
              "      <td>My boyfriend and i sat at the bar and had a co...</td>\n",
              "      <td>1</td>\n",
              "      <td>boyfriend sat bar completely delightful experi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>171</th>\n",
              "      <td>This really is how Vegas fine dining used to b...</td>\n",
              "      <td>1</td>\n",
              "      <td>really vega fine dining used right menu handed...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>456</th>\n",
              "      <td>The food was very good and I enjoyed every mou...</td>\n",
              "      <td>1</td>\n",
              "      <td>food good enjoyed every mouthful enjoyable rel...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>Very friendly staff.</td>\n",
              "      <td>1</td>\n",
              "      <td>friendly staff</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>754</th>\n",
              "      <td>Main thing I didn't enjoy is that the crowd is...</td>\n",
              "      <td>0</td>\n",
              "      <td>main thing didnt enjoy crowd older crowd aroun...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>507</th>\n",
              "      <td>The holster that arrived did not match the pho...</td>\n",
              "      <td>0</td>\n",
              "      <td>holster arrived match photo ad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>351</th>\n",
              "      <td>He was terrible!</td>\n",
              "      <td>0</td>\n",
              "      <td>terrible</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>579</th>\n",
              "      <td>The budget was evidently very limited.</td>\n",
              "      <td>0</td>\n",
              "      <td>budget evidently limited</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>811</th>\n",
              "      <td>This film offers many delights and surprises.</td>\n",
              "      <td>1</td>\n",
              "      <td>film offer many delight surprise</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>966</th>\n",
              "      <td>however, my girl was complain that some time t...</td>\n",
              "      <td>0</td>\n",
              "      <td>however girl complain time phone doesnt wake l...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  text  ...                                              short\n",
              "873  My boyfriend and i sat at the bar and had a co...  ...  boyfriend sat bar completely delightful experi...\n",
              "171  This really is how Vegas fine dining used to b...  ...  really vega fine dining used right menu handed...\n",
              "456  The food was very good and I enjoyed every mou...  ...  food good enjoyed every mouthful enjoyable rel...\n",
              "198                               Very friendly staff.  ...                                     friendly staff\n",
              "754  Main thing I didn't enjoy is that the crowd is...  ...  main thing didnt enjoy crowd older crowd aroun...\n",
              "507  The holster that arrived did not match the pho...  ...                     holster arrived match photo ad\n",
              "351                                   He was terrible!  ...                                           terrible\n",
              "579           The budget was evidently very limited.    ...                           budget evidently limited\n",
              "811    This film offers many delights and surprises.    ...                   film offer many delight surprise\n",
              "966  however, my girl was complain that some time t...  ...  however girl complain time phone doesnt wake l...\n",
              "\n",
              "[10 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbwjijVyLcgP"
      },
      "source": [
        "## Modelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-06-17T01:35:22.212854Z",
          "start_time": "2019-06-17T01:35:22.040284Z"
        },
        "id": "eJZpD903LcgQ"
      },
      "source": [
        "# helper function to show results and charts\n",
        "def show_summary_report(actual, prediction):\n",
        "\n",
        "    if isinstance(actual, pd.Series):\n",
        "        actual = actual.values\n",
        "    if actual.dtype.name == 'object':\n",
        "        actual = actual.astype(int)\n",
        "    if prediction.dtype.name == 'object':\n",
        "        prediction = prediction.astype(int)\n",
        "\n",
        "    accuracy_ = accuracy_score(actual, prediction)\n",
        "    precision_ = precision_score(actual, prediction)\n",
        "    recall_ = recall_score(actual, prediction)\n",
        "    roc_auc_ = roc_auc_score(actual, prediction)\n",
        "\n",
        "    print('Accuracy : %.4f [TP / N] Proportion of predicted labels that match the true labels. Best: 1, Worst: 0' % accuracy_)\n",
        "    print('Precision: %.4f [TP / (TP + FP)] Not to label a negative sample as positive.        Best: 1, Worst: 0' % precision_)\n",
        "    print('Recall   : %.4f [TP / (TP + FN)] Find all the positive samples.                     Best: 1, Worst: 0' % recall_)\n",
        "    print('ROC AUC  : %.4f                                                                     Best: 1, Worst: < 0.5' % roc_auc_)\n",
        "    print('-' * 107)\n",
        "    print('TP: True Positives, FP: False Positives, TN: True Negatives, FN: False Negatives, N: Number of samples')\n",
        "\n",
        "    # Confusion Matrix\n",
        "    mat = confusion_matrix(actual, prediction)\n",
        "\n",
        "    # Precision/Recall\n",
        "    precision, recall, _ = precision_recall_curve(actual, prediction)\n",
        "    average_precision = average_precision_score(actual, prediction)\n",
        "    \n",
        "    # Compute ROC curve and ROC area\n",
        "    fpr, tpr, _ = roc_curve(actual, prediction)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    # plot\n",
        "    fig, ax = plt.subplots(1, 3, figsize = (18, 6))\n",
        "    fig.subplots_adjust(left = 0.02, right = 0.98, wspace = 0.2)\n",
        "\n",
        "    # Confusion Matrix\n",
        "    sns.heatmap(mat.T, square = True, annot = True, fmt = 'd', cbar = False, cmap = 'Blues', ax = ax[0])\n",
        "\n",
        "    ax[0].set_title('Confusion Matrix')\n",
        "    ax[0].set_xlabel('True label')\n",
        "    ax[0].set_ylabel('Predicted label')\n",
        "    \n",
        "    # Precision/Recall\n",
        "    step_kwargs = {'step': 'post'}\n",
        "    ax[1].step(recall, precision, color = 'b', alpha = 0.2, where = 'post')\n",
        "    ax[1].fill_between(recall, precision, alpha = 0.2, color = 'b', **step_kwargs)\n",
        "    ax[1].set_ylim([0.0, 1.0])\n",
        "    ax[1].set_xlim([0.0, 1.0])\n",
        "    ax[1].set_xlabel('Recall')\n",
        "    ax[1].set_ylabel('Precision')\n",
        "    ax[1].set_title('2-class Precision-Recall curve')\n",
        "\n",
        "    # ROC\n",
        "    ax[2].plot(fpr, tpr, color = 'darkorange', lw = 2, label = 'ROC curve (AUC = %0.2f)' % roc_auc)\n",
        "    ax[2].plot([0, 1], [0, 1], color = 'navy', lw = 2, linestyle = '--')\n",
        "    ax[2].set_xlim([0.0, 1.0])\n",
        "    ax[2].set_ylim([0.0, 1.0])\n",
        "    ax[2].set_xlabel('False Positive Rate')\n",
        "    ax[2].set_ylabel('True Positive Rate')\n",
        "    ax[2].set_title('Receiver Operating Characteristic')\n",
        "    ax[2].legend(loc = 'lower right')\n",
        "\n",
        "    plt.show()\n",
        "    \n",
        "    return (accuracy_, precision_, recall_, roc_auc_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-06-17T01:35:24.658233Z",
          "start_time": "2019-06-17T01:35:24.649227Z"
        },
        "id": "Hj2aoBqqLcgV"
      },
      "source": [
        "# Features and Labels\n",
        "X = df['short']\n",
        "y = df['sentiment']\n",
        "\n",
        "# split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yr_VmeNMLcgY"
      },
      "source": [
        "## Use Bag-of-Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-06-17T01:35:32.373670Z",
          "start_time": "2019-06-17T01:35:32.369681Z"
        },
        "id": "Rhd__LD6LcgZ"
      },
      "source": [
        "# create a matrix of word counts from the text\n",
        "counts = CountVectorizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-06-17T01:35:35.842101Z",
          "start_time": "2019-06-17T01:35:35.784219Z"
        },
        "id": "23CpVgPxLcgb"
      },
      "source": [
        "# do the actual counting\n",
        "A = counts.fit_transform(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-06-17T01:35:38.590493Z",
          "start_time": "2019-06-17T01:35:38.586469Z"
        },
        "id": "c_rue57RLcgd"
      },
      "source": [
        "# create a classifier using SVC\n",
        "classifier = SVC(kernel='linear', probability=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-06-17T01:35:41.929126Z",
          "start_time": "2019-06-17T01:35:41.745617Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lou4xDLmLcgh",
        "outputId": "12dde990-ca94-49f3-bc71-43ed394481f8"
      },
      "source": [
        "# train the classifier with the training data\n",
        "classifier.fit(A.toarray(), y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
              "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
              "    verbose=False)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-06-17T01:35:47.210207Z",
          "start_time": "2019-06-17T01:35:47.199250Z"
        },
        "id": "inkg1KTiLcgi"
      },
      "source": [
        "# do the transformation for the test data\n",
        "# NOTE: use `transform()` instead of `fit_transform()`\n",
        "B = counts.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-06-17T01:35:51.223067Z",
          "start_time": "2019-06-17T01:35:51.209096Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dg-HpdJ0Lcgk",
        "outputId": "fef44ec6-4000-4aa9-dd7b-79dfc37be548"
      },
      "source": [
        "# make predictions based on the test data\n",
        "predictions = classifier.predict(B.toarray())\n",
        "predictions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,\n",
              "       0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n",
              "       1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,\n",
              "       1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,\n",
              "       0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
              "       1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
              "       1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
              "       1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
              "       1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
              "       1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,\n",
              "       1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
              "       1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
              "       0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
              "       0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,\n",
              "       1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,\n",
              "       0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
              "       0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n",
              "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,\n",
              "       0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,\n",
              "       1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,\n",
              "       1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n",
              "       1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
              "       1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,\n",
              "       0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n",
              "       0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
              "       1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,\n",
              "       0, 0, 0, 1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-06-17T01:35:54.779047Z",
          "start_time": "2019-06-17T01:35:54.771069Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0HJn9qhLcgm",
        "outputId": "feff3fc9-7c21-4dd3-e70f-555f43e68a83"
      },
      "source": [
        "# check the accuracy\n",
        "print('Accuracy: %.4f' % accuracy_score(y_test, predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7983\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-Ia6a8ULcgn"
      },
      "source": [
        "## Repeat using TF-IDF\n",
        "TF-IDF = Term Frequency - Inverse Document Frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-06-17T01:36:02.927008Z",
          "start_time": "2019-06-17T01:36:02.785387Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "7Tg1dwSpLcgo",
        "outputId": "19aa408f-571a-44c8-c9de-10efcbce4da8"
      },
      "source": [
        "# create a matrix of word counts from the text\n",
        "# use TF-IDF\n",
        "tfidf = TfidfVectorizer()\n",
        "# do the actual counting\n",
        "A = tfidf.fit_transform(X_train, y_train)\n",
        "\n",
        "# train the classifier with the training data\n",
        "classifier.fit(A.toarray(), y_train)\n",
        "\n",
        "# do the transformation for the test data\n",
        "# NOTE: use `transform()` instead of `fit_transform()`\n",
        "B = tfidf.transform(X_test)\n",
        "\n",
        "# make predictions based on the test data\n",
        "predictions = classifier.predict(B)\n",
        "\n",
        "# check the accuracy\n",
        "print('Accuracy: %.4f' % accuracy_score(y_test, predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-4a9e36701804>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# make predictions based on the test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# check the accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \"\"\"\n\u001b[0;32m--> 315\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_for_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse_predict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dense_predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_validate_for_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    454\u001b[0m             raise ValueError(\n\u001b[1;32m    455\u001b[0m                 \u001b[0;34m\"cannot use sparse input in %r trained on dense data\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m                 % type(self).__name__)\n\u001b[0m\u001b[1;32m    457\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot use sparse input in 'SVC' trained on dense data"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyzAOn1cZ0n-"
      },
      "source": [
        "show_summary_report(y_test, predictions_tfidf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5PTu402Lcgq"
      },
      "source": [
        "## Repeating it all for comparision\n",
        "Repeating the whole lot in one big block\n",
        "\n",
        "Find 'Accuracy', 'Precision', 'Recall', 'ROC_AUC' using CountVectorizer and TfidfVectorizer and keep the result in a dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-06-17T01:37:30.200048Z",
          "start_time": "2019-06-17T01:37:30.197044Z"
        },
        "id": "_98CzdfPLcgq"
      },
      "source": [
        "# ANSWER"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RERADKgNFq9T"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "> > > > > > > > > © 2021 Institute of Data\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}